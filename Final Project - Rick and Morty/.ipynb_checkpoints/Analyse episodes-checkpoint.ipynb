{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26fa40b",
   "metadata": {},
   "source": [
    "## Extract the synopsis and plot of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "58a428cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "332989f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "611f39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(aText):\n",
    "    # remove [[file:<aFile>]]\n",
    "    pattern_file = \"\\[\\[file\\:.*?\\]\\]\"\n",
    "    cleanText = re.sub(pattern_file,'', aText)\n",
    "    # replace then remove [https://<link>]\n",
    "    pattern_url = \"\\[http.*? (.*?)\\]\"\n",
    "    cleanText = re.sub(pattern_url,r'\\1', cleanText)\n",
    "    cleanText = re.sub(\"\\[http.*?\\]\",'',cleanText)\n",
    "    # replace [[<aCharacter>|<infos>]] by <aCharacter>\n",
    "    pattern = \"\\[\\[(.*?)(?:\\|.*?)?\\]\\]\"\n",
    "    cleanText = re.sub(pattern,r\"\\1\", cleanText)\n",
    "    # replace remained [,]\n",
    "    cleanText = cleanText.replace('[','')\n",
    "    cleanText = cleanText.replace(']','')\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e137827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path = 'episode_pages'\n",
    "all_files = [f for f in listdir(pages_path) if isfile(join(pages_path, f))]\n",
    "\n",
    "# for each episode\n",
    "all_descriptions = {}\n",
    "synopsis_header, plot_header = \"==Synopsis==\", \"==Plot==\"\n",
    "\n",
    "for aFile in all_files:\n",
    "    # Read the file containing the episode's page\n",
    "    episode_page = open(pages_path+'/'+aFile, encoding=\"utf-8\").read() \n",
    "    print('Processing... ', aFile)\n",
    "    # Replace if needed synopsis and plot headers\n",
    "    episode_page = re.sub(\"\\=\\= ?Synopsis ?\\=\\=\",\"==Synopsis==\",episode_page)\n",
    "    episode_page = re.sub(\"\\=\\= ?Plot ?\\=\\=\",\"==Plot==\",episode_page)\n",
    "    # Initialize\n",
    "    synopsis, plot = '',''\n",
    "    # Get synopsis and plot if found in page\n",
    "    if synopsis_header in episode_page:\n",
    "        synopsis = episode_page.split(synopsis_header)[1].split('==')[0]\n",
    "        print(\"   Synopsis found\")\n",
    "    if plot_header in episode_page:\n",
    "        plot = episode_page.split(plot_header)[1].split('==')[0]\n",
    "        print(\"   Plot found\")\n",
    "    # Clean synopsis and plot\n",
    "    synopsis, plot = clean_text(synopsis), clean_text(plot)\n",
    "    # Merge both texts\n",
    "    story = synopsis+plot\n",
    "    # Save it locally\n",
    "    # f = codecs.open('episode_stories/'+aFile, \"w+\", \"utf-8\")\n",
    "    # f.write(story)\n",
    "    # f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55024e",
   "metadata": {},
   "source": [
    "## Tokenization of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "db8593ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3aa668c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path = 'episode_stories'\n",
    "all_files = [f for f in listdir(pages_path) if isfile(join(pages_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d7cf0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stopwords list in given language\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "# tokenization factory\n",
    "tk = WordPunctTokenizer()\n",
    "# lemmatization factory\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "\n",
    "# for each episode story\n",
    "for aFile in all_files:\n",
    "    # Read the file containing the character's page description.\n",
    "    episode_page = open(pages_path+'/'+aFile, encoding=\"utf-8\").read() \n",
    "    # Set everything to lower case.\n",
    "    episode_page = episode_page.lower()\n",
    "    # Tokenize your text\n",
    "    episode_page = tk.tokenize(episode_page)\n",
    "    # Exclude punctuation and stop words\n",
    "    episode_page = [aToken for aToken in episode_page if aToken.isalnum() and aToken not in stopwords]\n",
    "    # Lemmatize words\n",
    "    episode_page = [ lm.lemmatize(w) for w in episode_page ]\n",
    "    # Remove words with less than 2 letters\n",
    "    episode_page = [ w for w in episode_page if len(w)>2]\n",
    "    # Transform list into list separated by spaces\n",
    "    episode_page = ''.join([str(elem)+' ' for elem in episode_page])\n",
    "    # Save your output for future use\n",
    "    # f = codecs.open('episode_tokens/'+aFile, \"w+\", \"utf-8\")\n",
    "    # f.write(episode_page)\n",
    "    # f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cbaa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
