{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3447406b",
   "metadata": {},
   "source": [
    "## Extract the synopsis and plot of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9f61a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04efe366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(aText):\n",
    "    # remove [[file:<aFile>]]\n",
    "    pattern_file = \"\\[\\[file\\:.*?\\]\\]\"\n",
    "    cleanText = re.sub(pattern_file,'', aText)\n",
    "    # replace then remove [https://<link>]\n",
    "    pattern_url = \"\\[http.*? (.*?)\\]\"\n",
    "    cleanText = re.sub(pattern_url,r'\\1', cleanText)\n",
    "    cleanText = re.sub(\"\\[http.*?\\]\",'',cleanText)\n",
    "    # replace [[<aCharacter>|<infos>]] by <aCharacter>\n",
    "    pattern = \"\\[\\[(.*?)(?:\\|.*?)?\\]\\]\"\n",
    "    cleanText = re.sub(pattern,r\"\\1\", cleanText)\n",
    "    # replace remained [,]\n",
    "    cleanText = cleanText.replace('[','')\n",
    "    cleanText = cleanText.replace(']','')\n",
    "    return cleanText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09417f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path = 'episode_pages'\n",
    "all_files = [f for f in listdir(pages_path) if isfile(join(pages_path, f))]\n",
    "\n",
    "# for each episode\n",
    "all_descriptions = {}\n",
    "synopsis_header, plot_header = \"==Synopsis==\", \"==Plot==\"\n",
    "\n",
    "for aFile in all_files:\n",
    "    # Read the file containing the episode's page\n",
    "    episode_page = open(pages_path+'/'+aFile, encoding=\"utf-8\").read() \n",
    "    print('Processing... ', aFile)\n",
    "    # Replace if needed synopsis and plot headers\n",
    "    episode_page = re.sub(\"\\=\\= ?Synopsis ?\\=\\=\",\"==Synopsis==\",episode_page)\n",
    "    episode_page = re.sub(\"\\=\\= ?Plot ?\\=\\=\",\"==Plot==\",episode_page)\n",
    "    # Initialize\n",
    "    synopsis, plot = '',''\n",
    "    # Get synopsis and plot if found in page\n",
    "    if synopsis_header in episode_page:\n",
    "        synopsis = episode_page.split(synopsis_header)[1].split('==')[0]\n",
    "        print(\"   Synopsis found\")\n",
    "    if plot_header in episode_page:\n",
    "        plot = episode_page.split(plot_header)[1].split('==')[0]\n",
    "        print(\"   Plot found\")\n",
    "    # Clean synopsis and plot\n",
    "    synopsis, plot = clean_text(synopsis), clean_text(plot)\n",
    "    # Merge both texts\n",
    "    story = synopsis+plot\n",
    "    # Save it locally\n",
    "    # f = codecs.open('episode_stories/'+aFile, \"w+\", \"utf-8\")\n",
    "    # f.write(story)\n",
    "    # f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f1269",
   "metadata": {},
   "source": [
    "## Tokenization of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c39ae8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eef7f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path = 'episode_stories'\n",
    "all_files = [f for f in listdir(pages_path) if isfile(join(pages_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "061eac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_characters = pd.read_csv('RaM_characters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3d6cd306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the stopwords list in given language\n",
    "stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
    "# tokenization factory\n",
    "tk = WordPunctTokenizer()\n",
    "# lemmatization factory\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "# to exclude\n",
    "specific_words = ['smith','sanchez']\n",
    "\n",
    "# for each episode story\n",
    "for aFile in all_files:\n",
    "    # Read the file containing the character's page description.\n",
    "    episode_page = open(pages_path+'/'+aFile, encoding=\"utf-8\").read() \n",
    "    # Set everything to lower case.\n",
    "    episode_page = episode_page.lower()\n",
    "    # Exclude characters names, BEFORE tokenisation\n",
    "    for aCharacterName in specific_words:\n",
    "        episode_page = episode_page.replace(aCharacterName.lower(),'')\n",
    "    # Tokenize your text\n",
    "    episode_page = tk.tokenize(episode_page)\n",
    "    # Exclude punctuation and stop words\n",
    "    episode_page = [aToken for aToken in episode_page if aToken.isalnum() and aToken not in stopwords]\n",
    "    # Lemmatize words\n",
    "    episode_page = [ lm.lemmatize(w) for w in episode_page ]\n",
    "    # Remove words with less than 2 letters\n",
    "    episode_page = [ w for w in episode_page if len(w)>2]\n",
    "    # Transform list into list separated by spaces\n",
    "    episode_page = ''.join([str(elem)+' ' for elem in episode_page])\n",
    "    # Save your output for future use\n",
    "    f = codecs.open('episode_tokens/'+aFile, \"w+\", \"utf-8\")\n",
    "    f.write(episode_page)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7abb29",
   "metadata": {},
   "source": [
    "# Topic Detection\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7fcd7bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "38f6735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_path = 'episode_tokens'\n",
    "all_files = [f for f in listdir(pages_path) if isfile(join(pages_path, f))]\n",
    "\n",
    "# get the dictionary of tokens for each episode\n",
    "all_tokens = {}\n",
    "\n",
    "# for each episode tokens list\n",
    "for aFile in all_files:\n",
    "    # Read the file containing the tokens of the episode\n",
    "    episode_tokens = open(pages_path+'/'+aFile, encoding=\"utf-8\").read() \n",
    "    all_tokens[aFile]= episode_tokens.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6d269f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dictionary id2word by using corpora.Dictionary(YOUR_LIST_OF_LISTS)\n",
    "dictionary = gensim.corpora.Dictionary(all_tokens.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "8e4e86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out tokens that appear in less than 15 documents (absolute number) \n",
    "# or more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "70e6305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words and how many times those words appear = bag of words\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in all_tokens.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0f3cde61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>Title</th>\n",
       "      <th>rate</th>\n",
       "      <th>nb_votes</th>\n",
       "      <th>imdb_link</th>\n",
       "      <th>Season_nb</th>\n",
       "      <th>Episode_nb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tales From the Citadel</td>\n",
       "      <td>9.8</td>\n",
       "      <td>29698</td>\n",
       "      <td>https://www.imdb.com/title/tt5218332/</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Rickshank Rickdemption</td>\n",
       "      <td>9.6</td>\n",
       "      <td>19948</td>\n",
       "      <td>https://www.imdb.com/title/tt5218228/</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Total Rickall</td>\n",
       "      <td>9.6</td>\n",
       "      <td>16633</td>\n",
       "      <td>https://www.imdb.com/title/tt4832262/</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Rickmurai Jack</td>\n",
       "      <td>9.5</td>\n",
       "      <td>8127</td>\n",
       "      <td>https://www.imdb.com/title/tt15041334/</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Vat of Acid Episode</td>\n",
       "      <td>9.5</td>\n",
       "      <td>12058</td>\n",
       "      <td>https://www.imdb.com/title/tt10655692/</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                       Title  rate  nb_votes  \\\n",
       "0     1      Tales From the Citadel   9.8     29698   \n",
       "1     2  The Rickshank Rickdemption   9.6     19948   \n",
       "2     3               Total Rickall   9.6     16633   \n",
       "3     4              Rickmurai Jack   9.5      8127   \n",
       "4     5     The Vat of Acid Episode   9.5     12058   \n",
       "\n",
       "                                imdb_link  Season_nb  Episode_nb  \n",
       "0   https://www.imdb.com/title/tt5218332/          3           7  \n",
       "1   https://www.imdb.com/title/tt5218228/          3           1  \n",
       "2   https://www.imdb.com/title/tt4832262/          2           4  \n",
       "3  https://www.imdb.com/title/tt15041334/          5          10  \n",
       "4  https://www.imdb.com/title/tt10655692/          4           8  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_episodes = pd.read_csv(\"RaM_episodes.csv\")\n",
    "df_ep_ranking = pd.read_csv('RaM_imdb_episodes_ranking.csv')\n",
    "df_ep_ranking.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5b638e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seasonNb_and_episodeNb(aFileEpisodeName):\n",
    "    return int(aFileEpisodeName[1:3]), int(aFileEpisodeName[4:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "cda79d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top10 words for S01E05 (Meeseeks and Destroy):\n",
      " 'adventure' appears 12 times.\n",
      " 'help' appears 7 times.\n",
      " 'quickly' appears 6 times.\n",
      " 'hand' appears 5 times.\n",
      " 'lead' appears 5 times.\n",
      " 'creature' appears 4 times.\n",
      " 'portal' appears 4 times.\n",
      " 'point' appears 3 times.\n",
      " 'world' appears 3 times.\n",
      " 'having' appears 2 times.\n"
     ]
    }
   ],
   "source": [
    "# get a preview for a chosen episode\n",
    "episode_to_preview = 'S01E05'\n",
    "ep_index = np.argwhere(np.array(all_files)==episode_to_preview+'.txt')[0][0]\n",
    "\n",
    "seasonNb, epNb = get_seasonNb_and_episodeNb(episode_to_preview)\n",
    "ep_title = list(df_episodes[(df_episodes.Season_nb == seasonNb) & (df_episodes.Episode_nb == epNb)].Title)[0]\n",
    "print(\"Top10 words for {} ({}):\".format(episode_to_preview, ep_title))\n",
    "\n",
    "bow_doc_ep = list(sorted(bow_corpus[ep_index], reverse=True, key=lambda x:x[1]))\n",
    "for i in range(10):\n",
    "    print(\" '{}' appears {} times.\".format(\n",
    "        dictionary[bow_doc_ep[i][0]], \n",
    "        bow_doc_ep[i][1])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eb7b8f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b34df4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.054*\"planet\" + 0.035*\"adventure\" + 0.032*\"quickly\" + 0.030*\"help\" + 0.029*\"garage\" + 0.021*\"fight\" + 0.021*\"explains\" + 0.017*\"travel\" + 0.017*\"ship\" + 0.016*\"open\"\n",
      "Topic: 1 \n",
      "Words: 0.041*\"planet\" + 0.032*\"people\" + 0.029*\"world\" + 0.027*\"life\" + 0.027*\"ship\" + 0.026*\"car\" + 0.022*\"room\" + 0.022*\"stop\" + 0.021*\"escape\" + 0.021*\"killing\"\n",
      "Topic: 2 \n",
      "Words: 0.034*\"portal\" + 0.029*\"planet\" + 0.026*\"new\" + 0.022*\"gun\" + 0.021*\"going\" + 0.020*\"despite\" + 0.020*\"return\" + 0.020*\"want\" + 0.019*\"free\" + 0.019*\"life\"\n",
      "Topic: 3 \n",
      "Words: 0.039*\"fight\" + 0.036*\"body\" + 0.033*\"idea\" + 0.032*\"world\" + 0.029*\"control\" + 0.029*\"space\" + 0.027*\"head\" + 0.025*\"portal\" + 0.022*\"notice\" + 0.022*\"lead\"\n",
      "Topic: 4 \n",
      "Words: 0.068*\"save\" + 0.055*\"device\" + 0.037*\"head\" + 0.035*\"point\" + 0.028*\"death\" + 0.024*\"planet\" + 0.021*\"appears\" + 0.019*\"instead\" + 0.019*\"new\" + 0.017*\"travel\"\n",
      "Topic: 5 \n",
      "Words: 0.034*\"room\" + 0.031*\"space\" + 0.025*\"know\" + 0.022*\"having\" + 0.020*\"adventure\" + 0.019*\"plan\" + 0.018*\"use\" + 0.017*\"stop\" + 0.016*\"getting\" + 0.016*\"open\"\n",
      "Topic: 6 \n",
      "Words: 0.053*\"planet\" + 0.028*\"dimension\" + 0.028*\"point\" + 0.026*\"house\" + 0.025*\"help\" + 0.025*\"return\" + 0.025*\"creature\" + 0.023*\"portal\" + 0.023*\"death\" + 0.023*\"room\"\n",
      "Topic: 7 \n",
      "Words: 0.060*\"dimension\" + 0.041*\"portal\" + 0.032*\"explains\" + 0.031*\"shoot\" + 0.030*\"gun\" + 0.030*\"house\" + 0.029*\"room\" + 0.024*\"start\" + 0.022*\"having\" + 0.022*\"space\"\n",
      "Topic: 8 \n",
      "Words: 0.038*\"want\" + 0.036*\"new\" + 0.035*\"body\" + 0.028*\"house\" + 0.024*\"car\" + 0.023*\"death\" + 0.021*\"soon\" + 0.020*\"garage\" + 0.020*\"ship\" + 0.019*\"relationship\"\n",
      "Topic: 9 \n",
      "Words: 0.063*\"portal\" + 0.053*\"car\" + 0.030*\"space\" + 0.028*\"gun\" + 0.027*\"shoot\" + 0.022*\"ship\" + 0.022*\"adventure\" + 0.021*\"return\" + 0.021*\"death\" + 0.018*\"telling\"\n"
     ]
    }
   ],
   "source": [
    "# Running LDA using Bag of Words\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b0ea7dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Word: 0.047*\"creature\" + 0.022*\"relationship\" + 0.021*\"world\" + 0.020*\"dimension\" + 0.017*\"head\" + 0.017*\"place\" + 0.017*\"new\" + 0.017*\"life\" + 0.017*\"device\" + 0.017*\"want\"\n",
      "Topic: 1 \n",
      "Word: 0.060*\"portal\" + 0.047*\"dimension\" + 0.043*\"gun\" + 0.033*\"return\" + 0.021*\"head\" + 0.021*\"house\" + 0.020*\"new\" + 0.019*\"shoot\" + 0.018*\"save\" + 0.015*\"killing\"\n",
      "Topic: 2 \n",
      "Word: 0.058*\"inside\" + 0.028*\"house\" + 0.023*\"making\" + 0.019*\"friend\" + 0.019*\"named\" + 0.019*\"fight\" + 0.018*\"garage\" + 0.018*\"attack\" + 0.018*\"going\" + 0.018*\"car\"\n",
      "Topic: 3 \n",
      "Word: 0.037*\"house\" + 0.034*\"car\" + 0.032*\"body\" + 0.025*\"space\" + 0.023*\"free\" + 0.023*\"talking\" + 0.020*\"look\" + 0.020*\"want\" + 0.018*\"killed\" + 0.018*\"idea\"\n",
      "Topic: 4 \n",
      "Word: 0.036*\"soon\" + 0.030*\"travel\" + 0.029*\"body\" + 0.029*\"death\" + 0.026*\"world\" + 0.023*\"inside\" + 0.020*\"new\" + 0.020*\"adventure\" + 0.019*\"causing\" + 0.017*\"way\"\n",
      "Topic: 5 \n",
      "Word: 0.039*\"planet\" + 0.023*\"ship\" + 0.022*\"open\" + 0.019*\"work\" + 0.017*\"know\" + 0.017*\"people\" + 0.017*\"saying\" + 0.017*\"space\" + 0.016*\"explains\" + 0.016*\"order\"\n",
      "Topic: 6 \n",
      "Word: 0.034*\"car\" + 0.032*\"device\" + 0.030*\"eventually\" + 0.029*\"planet\" + 0.026*\"realizes\" + 0.025*\"telling\" + 0.023*\"portal\" + 0.022*\"later\" + 0.020*\"causing\" + 0.020*\"gun\"\n",
      "Topic: 7 \n",
      "Word: 0.063*\"creature\" + 0.044*\"want\" + 0.018*\"watch\" + 0.017*\"portal\" + 0.017*\"room\" + 0.015*\"left\" + 0.014*\"space\" + 0.013*\"having\" + 0.013*\"body\" + 0.013*\"new\"\n",
      "Topic: 8 \n",
      "Word: 0.024*\"set\" + 0.024*\"watch\" + 0.024*\"world\" + 0.024*\"want\" + 0.023*\"death\" + 0.022*\"dimension\" + 0.022*\"room\" + 0.021*\"having\" + 0.020*\"escape\" + 0.018*\"left\"\n",
      "Topic: 9 \n",
      "Word: 0.022*\"dimension\" + 0.022*\"planet\" + 0.016*\"having\" + 0.016*\"explains\" + 0.015*\"fly\" + 0.014*\"named\" + 0.014*\"later\" + 0.014*\"stop\" + 0.013*\"going\" + 0.013*\"love\"\n"
     ]
    }
   ],
   "source": [
    "# Running LDA using TF-IDF\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} \\nWord: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847fbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d50a5fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 11 fields in line 542, saw 12\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9868/1955985188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_characters_with_attr_and_communities.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 11 fields in line 542, saw 12\n"
     ]
    }
   ],
   "source": [
    "pd.read_csv(\"df_characters_with_attr_and_communities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d77de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
